{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"ACTIVE\",\n",
      "  \"account\": \"bruzzese\",\n",
      "  \"account_type\": \"SERVICE\",\n",
      "  \"created_at\": \"2020-02-17T14:23:59\",\n",
      "  \"suspended_at\": null,\n",
      "  \"updated_at\": \"2020-02-17T14:23:59\",\n",
      "  \"deleted_at\": null,\n",
      "  \"email\": \"bruzzese@pic.es\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import sys,os,os.path,io,json,math,re,time,uuid,random,pytz\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Set Rucio virtual environment configuration \n",
    "\n",
    "os.environ['RUCIO_HOME']=os.path.expanduser('~/Rucio-v2/rucio')\n",
    "\n",
    "from rucio.client.client import *\n",
    "from rucio.client.rseclient import *\n",
    "from rucio.rse import rsemanager as rsemgr\n",
    "from rucio.client.client import Client\n",
    "from rucio.client.didclient import DIDClient\n",
    "from rucio.client.replicaclient import ReplicaClient\n",
    "from rucio.client.downloadclient import DownloadClient\n",
    "import rucio.rse.rsemanager as rsemgr\n",
    "from rucio.client.ruleclient import RuleClient\n",
    "from rucio.client.uploadclient import UploadClient\n",
    "from rucio.common.exception import (AccountNotFound, Duplicate, RucioException, DuplicateRule, InvalidObject, DataIdentifierAlreadyExists, FileAlreadyExists, RucioException,\n",
    "                                    AccessDenied, InsufficientAccountLimit, RuleNotFound, AccessDenied, InvalidRSEExpression,\n",
    "                                    InvalidReplicationRule, RucioException, DataIdentifierNotFound, InsufficientTargetRSEs,\n",
    "                                    ReplicationRuleCreationTemporaryFailed, InvalidRuleWeight, StagingAreaRuleRequiresLifetime)\n",
    "from rucio.common.utils import adler32, detect_client_location, execute, generate_uuid, md5, send_trace, GLOBALLY_SUPPORTED_CHECKSUMS\n",
    "\n",
    "sys.path.append(\"/usr/lib64/python3.6/site-packages/\")\n",
    "import gfal2\n",
    "from gfal2 import Gfal2Context, GError\n",
    "\n",
    "gfal = Gfal2Context()\n",
    "\n",
    "# Rucio settings \n",
    " \n",
    "RSE_origin = 'PIC-INJECT'\n",
    "RSE_destiny = 'PIC-DCACHE'\n",
    "RSE_destiny_2 = 'INFN-NA-DPM'\n",
    "RSE_QOS = 'QOS=FAST'\n",
    "\n",
    "RSEs = {'RSE_destiny':RSE_destiny, 'RSE_QOS':RSE_QOS, 'RSE_destiny_2':RSE_destiny_2}\n",
    "\n",
    "account = 'bruzzese'\n",
    "auth_type = 'x509_proxy'\n",
    "Default_Scope = 'MAGIC_PIC_BRUZZESE'\n",
    "Client = Client(account=account)\n",
    "uploadClient = UploadClient()\n",
    "rulesClient = RuleClient()\n",
    "downloadClient = DownloadClient()\n",
    "\n",
    "print(json.dumps(Client.whoami(), indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "\n",
    "# Functions\n",
    "\n",
    "############################\n",
    "\n",
    "def generate_random_file(size, copies = 1, filename=None):\n",
    "    \"\"\"\n",
    "    generate big binary file with the specified size in bytes\n",
    "    :param filename: the filename\n",
    "    :param size: the size in bytes\n",
    "    :param copies: number of output files to generate\n",
    "    \n",
    "    \"\"\"\n",
    "    n_files = []\n",
    "    n_files = np.array(n_files, dtype = np.float32)   \n",
    "    for i in range(copies):\n",
    "        \n",
    "        if filename == None :      \n",
    "            date = str(datetime.today().strftime('%Y%m%d'))\n",
    "            run = str(random.randint(10000000,99999999))\n",
    "            file = date + '_M1_' + run + '.005_D_1ES1959_650-W0.40_000.root'\n",
    "        \n",
    "        if os.path.exists(file) : \n",
    "            print (\"File %s already exist\" %file)\n",
    "\n",
    "        else:\n",
    "            print (\"File %s not exist\" %file)    \n",
    "            try : \n",
    "                newfile = open(file, \"wb\")\n",
    "                newfile.seek(size)\n",
    "                newfile.write(b\"\\0\")\n",
    "                newfile.close ()\n",
    "                os.stat(file).st_size\n",
    "                print('random file with size %f generated ok'%size)\n",
    "                n_files = np.append(n_files, file)\n",
    "            except :\n",
    "                print('could not be generate file %s'%file)\n",
    "\n",
    "    return(n_files)\n",
    "\n",
    "def look_for_run(fileName) :  \n",
    "\n",
    "    try :\n",
    "        run = re.search('\\d{8}\\.', fileName)\n",
    "        if not run :\n",
    "            run = re.search('_\\d{8}', fileName)\n",
    "            run = run[0].replace('_','')\n",
    "        elif (type(run).__module__, type(run).__name__) == ('_sre', 'SRE_Match') : \n",
    "            run = run.group(0)\n",
    "            run = run.replace('.','')\n",
    "        else :\n",
    "            run = run[0].replace('.','')\n",
    "            \n",
    "        return(str(run))\n",
    "    except : \n",
    "        pass\n",
    "    try :\n",
    "        if not run :\n",
    "            run = re.findall('\\d{8}\\_', fileName)\n",
    "            run = run[0].replace('_','')\n",
    "        return(str(run))\n",
    "    except : \n",
    "        pass\n",
    "    \n",
    "def createDataset(new_dataset, new_scope=Default_Scope) :         \n",
    "    try:\n",
    "        Client.add_dataset(scope=new_scope, name=new_dataset)\n",
    "        return(True)\n",
    "    except DataIdentifierAlreadyExists:\n",
    "        print(\"|  -  -  - Dataset %s already exists\" % new_dataset)\n",
    "        return(False)\n",
    "    except Duplicate as error:\n",
    "        return generate_http_error_flask(409, 'Duplicate', error.args[0])\n",
    "    except AccountNotFound as error:\n",
    "        return generate_http_error_flask(404, 'AccountNotFound', error.args[0])\n",
    "    except RucioException as error:\n",
    "        return generate_http_error_flask(500, error.__class__.__name__, error.args[0])\n",
    "\n",
    "def registerIntoGroup(n_file, new_dataset, new_scope=Default_Scope):\n",
    "\n",
    "    type_1 = Client.get_did(scope=new_scope, name=new_dataset)\n",
    "    type_2 = Client.get_did(scope=new_scope, name=n_file)\n",
    "\n",
    "    try:\n",
    "        Client.attach_dids(scope=new_scope, name=new_dataset, dids=[{'scope':new_scope, 'name':n_file}])\n",
    "    except :\n",
    "        PrintException()\n",
    "        \n",
    "def addReplicaRule(destRSE, group, new_scope=Default_Scope):\n",
    "\n",
    "    if destRSE:\n",
    "        try:\n",
    "            rule = rulesClient.add_replication_rule([{\"scope\":new_scope,\"name\":group}],copies=1, rse_expression=destRSE, grouping='ALL', account=account, purge_replicas=True, source_replica_expression=RSE_destiny)\n",
    "            return(rule[0])\n",
    "        \n",
    "        except DuplicateRule:\n",
    "            PrintException()\n",
    "            rules = list(Client.list_account_rules(account=account))\n",
    "            if rules : \n",
    "                for rule in rules :\n",
    "                    if rule['rse_expression'] == destRSE and rule['scope'] == scope and rule['name'] == group:\n",
    "                        print('| - - - - Rule already exists %s which contains the following DID %s:%s' % (rule['id'],scope, group))\n",
    "    \n",
    "def getchecksum(name_file): \n",
    "    try :\n",
    "        checksum = gfal.checksum(name_file, 'md5')\n",
    "    except : \n",
    "        checksum = gfal.checksum(os.path.join('file:///'+os.getcwd(), name_file), 'md5')       \n",
    "    return(checksum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "\n",
    "# Check existence of json File\n",
    "\n",
    "############################\n",
    "\n",
    "def json_write(data, filename='Rucio-bkp.json'): \n",
    "    with io.open(filename, 'w') as f: \n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "def json_check(json_file_name='Rucio-bkp.json') :\n",
    "    # checks if file exists\n",
    "    if not os.path.isfile(json_file_name) : \n",
    "        return(False)\n",
    "    \n",
    "    elif os.stat(json_file_name).st_size == 0 :\n",
    "        os.remove(json_file_name)\n",
    "        return(False)\n",
    "    \n",
    "    elif os.path.isfile(json_file_name) and os.access(json_file_name, os.R_OK) :\n",
    "        return(True)\n",
    "\n",
    "def stateCheck(json_file='Rucio-bkp.json'):\n",
    "      \n",
    "    with open(json_file) as f : \n",
    "        data_keys  = json.load(f)\n",
    "        return(data_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 20201026_M1_95000364.005_D_1ES1959_650-W0.40_000.root not exist\n",
      "random file with size 33.000000 generated ok\n",
      "['20201026_M1_95000364.005_D_1ES1959_650-W0.40_000.root']\n"
     ]
    }
   ],
   "source": [
    "list_files = generate_random_file(size=random.randint(10,99), copies=1)     \n",
    "\n",
    "print(list_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201026_M1_95000364.005_D_1ES1959_650-W0.40_000.root\n"
     ]
    }
   ],
   "source": [
    "result_dict = dict()\n",
    "for n in range(0, len(list_files)) :\n",
    "    \n",
    "    name_file = list_files[n]    \n",
    "    print(name_file)\n",
    "    \n",
    "    \"\"\"\n",
    "    generate a dictionary with the information for the upload\n",
    "    :param path: the filename\n",
    "    :param rse: the destination RSE name\n",
    "    :param did_scope: The scope of the DID.\n",
    "    :param lifetime: Seconds of DID lifetime \n",
    "    \"\"\"\n",
    "    \n",
    "    file = {'path': \"./\"+name_file, 'rse': RSE_destiny, 'did_scope': Default_Scope, 'lifetime':5}\n",
    "    \n",
    "    result_dict[name_file] = {} \n",
    "    result_dict[name_file]['Scope'] = Default_Scope\n",
    "    result_dict[name_file]['Replicated'] = {'Local' : {'registered': datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y-%m-%dT%H:%M:%SZ'),'checksum':getchecksum(name_file), 'path':os.path.join(os.getcwd(),name_file)}}\n",
    "\n",
    "    # Perform upload\n",
    "    client_upload = uploadClient.upload([file])\n",
    "\n",
    "    # Create a dataset\n",
    "    dataset_name = look_for_run(name_file)\n",
    "    \n",
    "    createDataset(new_dataset = dataset_name, new_scope = Default_Scope)\n",
    "    \n",
    "    registerIntoGroup(n_file = name_file, new_dataset = str(dataset_name))\n",
    "\n",
    "    for rse in RSEs :\n",
    "        # Contruct a dictionary with the destiny RSE \n",
    "        if name_file in result_dict :   \n",
    "            temp_dict = dict()\n",
    "            temp_dict[name_file] = {} \n",
    "            temp_dict[name_file]['Replicated'] = {RSEs[rse] : {'registered': datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y-%m-%dT%H:%M:%SZ')}}\n",
    "\n",
    "            result_dict[name_file]['Replicated'].update(temp_dict[name_file]['Replicated'])\n",
    "        \n",
    "        rule = addReplicaRule(destRSE = RSEs[rse], group = dataset_name, new_scope = Default_Scope)\n",
    "        \n",
    "        # update a rule so it 'll be automatically be deleted once it has been successfully replicated\n",
    "        update = Client.update_replication_rule(rule_id=rule, options={'lifetime': 60, 'child_rule_id':rule, 'purge_replicas':True})\n",
    "    \n",
    "    \n",
    "    if json_check() == True :\n",
    "        result_dict.update(stateCheck())\n",
    "        \n",
    "# Save the uploads, replication and time into a json file\n",
    "json_write(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download a file fomr a datalake, and perform a checksum. \n",
    "# Then save it in a json file\n",
    "for n in range(0, len(list_files)) :\n",
    "    \n",
    "    name_file = list_files[n]\n",
    "    download = downloadClient.download_dids(items=[{'did':'{}:{}'.format(Default_Scope,name_file)}], num_threads=2, trace_custom_fields={}, traces_copy_out=None)\n",
    "    \n",
    "    result_dict = stateCheck()\n",
    "    if json_check() == True :\n",
    "        if name_file in result_dict.keys() :\n",
    "            temp_dict = dict()\n",
    "            temp_dict[name_file] = {} \n",
    "            temp_dict[name_file]['Replicated'] = {download[0]['sources'][0]['rse'] : {'downloaded': datetime.utcnow().replace(tzinfo=pytz.utc).strftime('%Y-%m-%dT%H:%M:%SZ'),'checksum':gfal.checksum('file:///'+download[0]['dest_file_paths'][0],'md5')}}\n",
    "            result_dict[name_file]['Replicated'][download[0]['sources'][0]['rse']].update(temp_dict[name_file]['Replicated'][download[0]['sources'][0]['rse']])\n",
    "    json_write(result_dict)\n",
    "    os.remove(download[0]['dest_file_paths'][0]) \n",
    "    os.remove(result_dict[name_file]['Replicated']['Local']['path'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
